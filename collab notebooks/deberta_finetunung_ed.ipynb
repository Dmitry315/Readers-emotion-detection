{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "  train_df='./train/train_df.csv'\n",
    "  test_df='./test/test_df.csv'\n",
    "  target_cols=['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "  classes=5\n",
    "  model='microsoft/deberta-v3-base'\n",
    "  embedd_dim=768\n",
    "  criterion = 'mse' # ['crossentropy', 'mse', 'l1', 'focal']\n",
    "  main_metric = 'acc@1'\n",
    "  model_file = './models/ed_best_deberta_base.pt'\n",
    "  # just use it\n",
    "  apex=True\n",
    "  gradient_checkpointing=True\n",
    "  num_cycles=0.5\n",
    "  num_warmup_steps=0\n",
    "  epochs=10\n",
    "  encoder_lr=2e-5\n",
    "  decoder_lr=2e-5\n",
    "  min_lr=1e-6\n",
    "  eps=1e-6\n",
    "  betas=(0.9, 0.999)\n",
    "  batch_size=2\n",
    "  max_len=512\n",
    "  weight_decay=0.01\n",
    "  # gradient_accumulation_steps=1\n",
    "  max_grad_norm=1000\n",
    "  seed=42\n",
    "  scheduler='cosine' # ['linear', 'cosine']\n",
    "  batch_scheduler=True\n",
    "  #\n",
    "  colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.colab:\n",
    "  from google.collab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import os\n",
    "  os.chdir('/content/drive/MyDrive/lab/bert_finetune')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import tokenizers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available(): # для GPU отдельный seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(CFG.seed)\n",
    "# есть стохастические операции на GPU\n",
    "# сделаем их детерминированными для воспроизводимости\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "CFG.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(CFG.train_df, index_col='index')\n",
    "test_df = pd.read_csv(CFG.test_df, index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mortar assault leaves at least 18 dead</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goal delight for Sheva</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nigeria hostage feared dead is freed</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>66</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bombers kill shoppers</td>\n",
       "      <td>66</td>\n",
       "      <td>39</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vegetables, not fruit, slow brain decline</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text   0   1   2   3   4   5\n",
       "index                                                                   \n",
       "1         Mortar assault leaves at least 18 dead  22   2  60   0  64   0\n",
       "2                         Goal delight for Sheva   0   0   0  93   0  38\n",
       "3           Nigeria hostage feared dead is freed  18   0  52  66  20  65\n",
       "4                          Bombers kill shoppers  66  39  94   0  86   0\n",
       "5      Vegetables, not fruit, slow brain decline   0   0  25  26   2  46"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((train_df, test_df))\n",
    "df.columns = ['text', 'Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "df = df.drop('Disgust', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mortar assault leaves at least 18 dead</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.410959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438356</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goal delight for Sheva</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nigeria hostage feared dead is freed</td>\n",
       "      <td>0.081448</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bombers kill shoppers</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.382114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349593</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vegetables, not fruit, slow brain decline</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.464646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text     Anger      Fear   \n",
       "index                                                                  \n",
       "1         Mortar assault leaves at least 18 dead  0.150685  0.410959  \\\n",
       "2                         Goal delight for Sheva  0.000000  0.000000   \n",
       "3           Nigeria hostage feared dead is freed  0.081448  0.235294   \n",
       "4                          Bombers kill shoppers  0.268293  0.382114   \n",
       "5      Vegetables, not fruit, slow brain decline  0.000000  0.252525   \n",
       "\n",
       "            Joy   Sadness  Surprise  \n",
       "index                                \n",
       "1      0.000000  0.438356  0.000000  \n",
       "2      0.709924  0.000000  0.290076  \n",
       "3      0.298643  0.090498  0.294118  \n",
       "4      0.000000  0.349593  0.000000  \n",
       "5      0.262626  0.020202  0.464646  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[CFG.target_cols] = df[CFG.target_cols].apply(lambda it: it/it.sum() if it.sum() else 0, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, test_df = train_test_split(tmp, test_size=2/8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = df.iloc[:, -5:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10100774, 0.16337742, 0.28517377, 0.20631656, 0.24092451])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4]), array([110, 198, 442, 275, 225]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ds.argmax(axis=-1), return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = (df['text']).values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def get_text(self, idx):\n",
    "        # tokenization\n",
    "        inputs = self.cfg.tokenizer.encode_plus(\n",
    "            self.texts[idx], \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=CFG.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            # padding='longest',\n",
    "            truncation=True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "    \n",
    "    def get_labels(self, idx):\n",
    "        if CFG.criterion != 'crossentropy' and CFG.criterion != 'focal':\n",
    "           return torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return torch.tensor(self.labels[idx]).type(torch.LongTensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.get_text(idx)\n",
    "        label = self.get_labels(idx)\n",
    "        return inputs, label\n",
    "\n",
    "def collate(inputs):\n",
    "\t\t# reduce sequence length\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(CFG.model)\n",
    "        if CFG.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.clf =  nn.Linear(CFG.embedd_dim, CFG.classes)\n",
    "        if not CFG.criterion == 'crossentropy':\n",
    "            self.sm = nn.Softmax(dim=-1)\n",
    "        torch.nn.init.xavier_uniform_(self.clf.weight)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        # sequence has [CLF] token in the beginning\n",
    "        # bert() returns first vector as pooling of sentence\n",
    "        x = self.model(input_ids= input_id, attention_mask=mask)[0]\n",
    "        out = self.pool(x, mask)\n",
    "        out = self.clf(out)\n",
    "        if not CFG.criterion == 'crossentropy':\n",
    "            return self.sm(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop and metrics and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance, pearsonr\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y, pred):\n",
    "    APd = torch.mean(torch.tensor([pearsonr(pred[i], y[i])[0] for i in range(pred.size()[0])]))\n",
    "    APe = torch.mean(torch.tensor([pearsonr(pred[:, i], y[:, i])[0] for i in range(CFG.classes)]))\n",
    "    RMSED = torch.mean(torch.tensor([np.sqrt(nn.functional.mse_loss(pred[i:i+1, :], y[i:i+1, :])) for i in range(pred.size()[0])]))\n",
    "    WD = torch.mean(torch.tensor([wasserstein_distance(pred[i], y[i]) for i in range(pred.size()[0])]))\n",
    "\n",
    "    y = torch.argmax(y, dim=1)\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    acc_1 = accuracy_score(y, pred)\n",
    "\n",
    "\n",
    "    return {'acc@1': acc_1, 'APd': APd, 'APe': APe, 'RMSED': RMSED, 'WD': WD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = torch.exp(logpt)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.reduction == 'mean': \n",
    "          return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "          return loss.sum()\n",
    "        else:\n",
    "          raise NotImplementedError(f'Not implemented reduction: {self.reduction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TrainDataset(CFG, train_df)\n",
    "valid_ds = TrainDataset(CFG, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.criterion == 'crossentropy':\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "elif CFG.criterion == 'mse':\n",
    "  criterion = nn.MSELoss()\n",
    "elif CFG.criterion == 'l1':\n",
    "  criterion = nn.SmoothL1Loss()\n",
    "elif CFG.criterion == 'focal':\n",
    "  criterion = FocalLoss(5)\n",
    "else:\n",
    "  raise NotImplementedError('Change loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_parameters = get_optimizer_params(model,\n",
    "                                           encoder_lr=CFG.encoder_lr, \n",
    "                                           decoder_lr=CFG.decoder_lr,\n",
    "                                           weight_decay=CFG.weight_decay)\n",
    "optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)\n",
    "scheduler = get_scheduler(CFG, optimizer, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(CFG.device)\n",
    "criterion.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, criterion, train_loader, valid_loader, epochs, scheduler):\n",
    "  best_score = 0\n",
    "  # multiplies gradient so it won't vanish (torch use float16)\n",
    "  scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "  for e in range(epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "      inputs = collate(inputs)\n",
    "      # move inputs to device\n",
    "      mask = inputs['attention_mask'].to(CFG.device)\n",
    "      input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "      labels = labels.to(CFG.device)\n",
    "      # forward\n",
    "      with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "        y_preds = model(input_id, mask)\n",
    "        loss = criterion(y_preds, labels)\n",
    "      # calculate loss\n",
    "      train_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      # loss.backward()\n",
    "      scaler.scale(loss).backward()\n",
    "      # gradient clipping\n",
    "      grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "      scaler.step(optimizer)\n",
    "      scaler.update()\n",
    "      # optimizer.step()\n",
    "      if CFG.batch_scheduler:\n",
    "          scheduler.step()\n",
    "    train_loss = np.mean(train_loss)\n",
    "    # valid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      valid_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for inputs, labels in valid_loader:\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "          y_preds = model(input_id, mask)\n",
    "          loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        valid_loss.append(loss.detach().cpu().item())\n",
    "    valid_loss = np.mean(valid_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "    # save best model\n",
    "    if best_score < metrics[CFG.main_metric]:\n",
    "      torch.save(model.state_dict(), CFG.model_file)\n",
    "      best_score = metrics[CFG.main_metric]\n",
    "    print('best_score =', best_score)\n",
    "    print(f'EPOCH {e + 1}:, train_loss = {train_loss: .5f}, valid_loss = {valid_loss}', *[f'{name} = {value: .5f}' for name, value in metrics.items()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:29<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.496\n",
      "EPOCH 1:, train_loss =  0.04700, valid_loss = 0.03136499774538808 acc@1 =  0.49600 APd =  0.56061 APe =  0.54802 RMSED =  0.15600 WD =  0.08702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:26<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.556\n",
      "EPOCH 2:, train_loss =  0.02435, valid_loss = 0.03200747556097451 acc@1 =  0.55600 APd =  0.59740 APe =  0.60502 RMSED =  0.15488 WD =  0.08594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:26<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.592\n",
      "EPOCH 3:, train_loss =  0.01727, valid_loss = 0.026679768530090176 acc@1 =  0.59200 APd =  0.64190 APe =  0.64131 RMSED =  0.13999 WD =  0.07867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:24<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.592\n",
      "EPOCH 4:, train_loss =  0.01273, valid_loss = 0.026733946129088363 acc@1 =  0.56800 APd =  0.65639 APe =  0.64233 RMSED =  0.13894 WD =  0.07906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:23<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.592\n",
      "EPOCH 5:, train_loss =  0.00993, valid_loss = 0.026985653727832768 acc@1 =  0.58800 APd =  0.65803 APe =  0.64862 RMSED =  0.14061 WD =  0.07970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:24<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.608\n",
      "EPOCH 6:, train_loss =  0.00821, valid_loss = 0.025361768723953338 acc@1 =  0.60800 APd =  0.67336 APe =  0.66254 RMSED =  0.13606 WD =  0.07727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:26<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.608\n",
      "EPOCH 7:, train_loss =  0.00717, valid_loss = 0.025899361427282056 acc@1 =  0.60400 APd =  0.67337 APe =  0.66038 RMSED =  0.13741 WD =  0.07623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:26<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.608\n",
      "EPOCH 8:, train_loss =  0.00607, valid_loss = 0.02575140152983959 acc@1 =  0.59600 APd =  0.67433 APe =  0.65959 RMSED =  0.13637 WD =  0.07708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:27<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.608\n",
      "EPOCH 9:, train_loss =  0.00560, valid_loss = 0.02605147673083203 acc@1 =  0.59600 APd =  0.67175 APe =  0.65846 RMSED =  0.13714 WD =  0.07739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:27<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.608\n",
      "EPOCH 10:, train_loss =  0.00552, valid_loss = 0.025897975205369884 acc@1 =  0.59200 APd =  0.67317 APe =  0.65870 RMSED =  0.13669 WD =  0.07706\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "train_loop(model, optimizer, criterion, train_loader, valid_loader, CFG.epochs, scheduler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      test_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for step, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "          y_preds = model(input_id, mask)\n",
    "          loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        test_loss.append(loss.detach().cpu().item())\n",
    "    test_loss = np.mean(test_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "\n",
    "    print(f'Test metrics: test_loss={test_loss: 0.5f}', *[f'{name} = {value: 0.5f}' for name, value in metrics.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(CFG.model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TrainDataset(CFG, test_df)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: test_loss= 0.02536 acc@1 =  0.60800 APd =  0.67336 APe =  0.66254 RMSED =  0.13606 WD =  0.07727\n"
     ]
    }
   ],
   "source": [
    "test(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: test_loss= 0.02274 acc@1 =  0.66000 APd =  0.70952 APe =  0.69687 RMSED =  0.12617 WD =  0.07235\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
