{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "  train_df='./train/train_df.csv'\n",
    "  test_df='./test/test_df.csv'\n",
    "  target_cols=['Anger', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "  classes=5\n",
    "  model='bert-base-cased'\n",
    "  embedd_dim=768\n",
    "  criterion = 'mse' # ['crossentropy', 'mse', 'l1', 'focal']\n",
    "  main_metric = 'acc@1'\n",
    "  model_file = './models/ed_best_bert_base.pt'\n",
    "  # just use it\n",
    "  apex=True\n",
    "  gradient_checkpointing=True\n",
    "  num_cycles=0.5\n",
    "  num_warmup_steps=0\n",
    "  epochs=10\n",
    "  encoder_lr=1e-5\n",
    "  decoder_lr=1e-5\n",
    "  min_lr=1e-6\n",
    "  eps=1e-6\n",
    "  betas=(0.9, 0.999)\n",
    "  batch_size=4\n",
    "  max_len=512\n",
    "  weight_decay=0.01\n",
    "  # gradient_accumulation_steps=1\n",
    "  max_grad_norm=1000\n",
    "  seed=42\n",
    "  scheduler='cosine' # ['linear', 'cosine']\n",
    "  batch_scheduler=False\n",
    "  #\n",
    "  colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.colab:\n",
    "  from google.collab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import os\n",
    "  os.chdir('/content/drive/MyDrive/lab/bert_finetune')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import tokenizers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available(): # для GPU отдельный seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(CFG.seed)\n",
    "# есть стохастические операции на GPU\n",
    "# сделаем их детерминированными для воспроизводимости\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(CFG.train_df, index_col='index')\n",
    "test_df = pd.read_csv(CFG.test_df, index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mortar assault leaves at least 18 dead</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goal delight for Sheva</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nigeria hostage feared dead is freed</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>66</td>\n",
       "      <td>20</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bombers kill shoppers</td>\n",
       "      <td>66</td>\n",
       "      <td>39</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vegetables, not fruit, slow brain decline</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text   0   1   2   3   4   5\n",
       "index                                                                   \n",
       "1         Mortar assault leaves at least 18 dead  22   2  60   0  64   0\n",
       "2                         Goal delight for Sheva   0   0   0  93   0  38\n",
       "3           Nigeria hostage feared dead is freed  18   0  52  66  20  65\n",
       "4                          Bombers kill shoppers  66  39  94   0  86   0\n",
       "5      Vegetables, not fruit, slow brain decline   0   0  25  26   2  46"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((train_df, test_df))\n",
    "df.columns = ['text', 'Anger', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise']\n",
    "df = df.drop('Disgust', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mortar assault leaves at least 18 dead</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.410959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438356</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Goal delight for Sheva</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.709924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nigeria hostage feared dead is freed</td>\n",
       "      <td>0.081448</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.298643</td>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bombers kill shoppers</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.382114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349593</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Vegetables, not fruit, slow brain decline</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252525</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.464646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text     Anger      Fear   \n",
       "index                                                                  \n",
       "1         Mortar assault leaves at least 18 dead  0.150685  0.410959  \\\n",
       "2                         Goal delight for Sheva  0.000000  0.000000   \n",
       "3           Nigeria hostage feared dead is freed  0.081448  0.235294   \n",
       "4                          Bombers kill shoppers  0.268293  0.382114   \n",
       "5      Vegetables, not fruit, slow brain decline  0.000000  0.252525   \n",
       "\n",
       "            Joy   Sadness  Surprise  \n",
       "index                                \n",
       "1      0.000000  0.438356  0.000000  \n",
       "2      0.709924  0.000000  0.290076  \n",
       "3      0.298643  0.090498  0.294118  \n",
       "4      0.000000  0.349593  0.000000  \n",
       "5      0.262626  0.020202  0.464646  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[CFG.target_cols] = df[CFG.target_cols].apply(lambda it: it/it.sum() if it.sum() else 0, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp, valid_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, test_df = train_test_split(tmp, test_size=2/8, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = (df['text']).values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def get_text(self, idx):\n",
    "        # tokenization\n",
    "        inputs = self.cfg.tokenizer.encode_plus(\n",
    "            self.texts[idx], \n",
    "            return_tensors=None, \n",
    "            add_special_tokens=True, \n",
    "            max_length=CFG.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            # padding='longest',\n",
    "            truncation=True\n",
    "        )\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return inputs\n",
    "    \n",
    "    def get_labels(self, idx):\n",
    "        if CFG.criterion != 'crossentropy' and CFG.criterion != 'focal':\n",
    "           return torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return torch.tensor(self.labels[idx]).type(torch.LongTensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.get_text(idx)\n",
    "        label = self.get_labels(idx)\n",
    "        return inputs, label\n",
    "\n",
    "def collate(inputs):\n",
    "\t\t# reduce sequence length\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(CFG.model)\n",
    "        if CFG.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.clf =  nn.Linear(CFG.embedd_dim, CFG.classes)\n",
    "        if not CFG.criterion == 'crossentropy':\n",
    "            self.sm = nn.Softmax(dim=-1)\n",
    "        torch.nn.init.xavier_uniform_(self.clf.weight)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        # sequence has [CLF] token in the beginning\n",
    "        # bert() returns first vector as pooling of sentence\n",
    "        _, x = self.model(input_ids= input_id, attention_mask=mask, return_dict=False)\n",
    "        out = self.clf(x)\n",
    "        if not CFG.criterion == 'crossentropy':\n",
    "            return self.sm(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train loop and metrics and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance, pearsonr\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y, pred):\n",
    "    APd = torch.mean(torch.tensor([pearsonr(pred[i], y[i])[0] for i in range(pred.size()[0])]))\n",
    "    APe = torch.mean(torch.tensor([pearsonr(pred[:, i], y[:, i])[0] for i in range(CFG.classes)]))\n",
    "    RMSED = torch.mean(torch.tensor([np.sqrt(nn.functional.mse_loss(pred[i:i+1, :], y[i:i+1, :])) for i in range(pred.size()[0])]))\n",
    "    WD = torch.mean(torch.tensor([wasserstein_distance(pred[i], y[i]) for i in range(pred.size()[0])]))\n",
    "\n",
    "    y = torch.argmax(y, dim=1)\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    acc_1 = accuracy_score(y, pred)\n",
    "\n",
    "\n",
    "    return {'acc@1': acc_1, 'APd': APd, 'APe': APe, 'RMSED': RMSED, 'WD': WD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "         'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "    ]\n",
    "    return optimizer_parameters\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = torch.exp(logpt)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.reduction == 'mean': \n",
    "          return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "          return loss.sum()\n",
    "        else:\n",
    "          raise NotImplementedError(f'Not implemented reduction: {self.reduction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TrainDataset(CFG, train_df)\n",
    "valid_ds = TrainDataset(CFG, valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.criterion == 'crossentropy':\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "elif CFG.criterion == 'mse':\n",
    "  criterion = nn.MSELoss()\n",
    "elif CFG.criterion == 'l1':\n",
    "  criterion = nn.SmoothL1Loss()\n",
    "elif CFG.criterion == 'focal':\n",
    "  criterion = FocalLoss(5)\n",
    "else:\n",
    "  raise NotImplementedError('Change loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_parameters = get_optimizer_params(model,\n",
    "#                                            encoder_lr=CFG.encoder_lr, \n",
    "#                                            decoder_lr=CFG.decoder_lr,\n",
    "#                                            weight_decay=CFG.weight_decay)\n",
    "# optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "optimizer = AdamW(model.parameters(), lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)\n",
    "scheduler = get_scheduler(CFG, optimizer, num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(CFG.device)\n",
    "criterion.to(CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, criterion, train_loader, valid_loader, epochs, scheduler):\n",
    "  best_score = 0\n",
    "  # multiplies gradient so it won't vanish (torch use float16)\n",
    "  scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "  for e in range(epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "      inputs = collate(inputs)\n",
    "      # move inputs to device\n",
    "      mask = inputs['attention_mask'].to(CFG.device)\n",
    "      input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "      labels = labels.to(CFG.device)\n",
    "      # forward\n",
    "      with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "        y_preds = model(input_id, mask)\n",
    "        loss = criterion(y_preds, labels)\n",
    "      # calculate loss\n",
    "      train_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      # loss.backward()\n",
    "      scaler.scale(loss).backward()\n",
    "      # gradient clipping\n",
    "      grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "      scaler.step(optimizer)\n",
    "      scaler.update()\n",
    "      # optimizer.step()\n",
    "      if CFG.batch_scheduler:\n",
    "          scheduler.step()\n",
    "    train_loss = np.mean(train_loss)\n",
    "    # valid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      valid_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for inputs, labels in valid_loader:\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "          y_preds = model(input_id, mask)\n",
    "          loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        valid_loss.append(loss.detach().cpu().item())\n",
    "    valid_loss = np.mean(valid_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "    # save best model\n",
    "    if best_score < metrics[CFG.main_metric]:\n",
    "      torch.save(model.state_dict(), CFG.model_file)\n",
    "      best_score = metrics[CFG.main_metric]\n",
    "    print('best_score =', best_score)\n",
    "    print(f'EPOCH {e + 1}:, train_loss = {train_loss: .5f}, valid_loss = {valid_loss: .5f}', *[f'{name} = {value: .5f}' for name, value in metrics.items()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:18<00:00, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.396\n",
      "EPOCH 1:, train_loss =  0.05340, valid_loss =  0.04196 acc@1 =  0.39600 APd =  0.34826 APe =  0.43224 RMSED =  0.19380 WD =  0.13394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:17<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.528\n",
      "EPOCH 2:, train_loss =  0.03393, valid_loss =  0.03114 acc@1 =  0.52800 APd =  0.55859 APe =  0.55954 RMSED =  0.15773 WD =  0.09698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:14<00:00, 12.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.532\n",
      "EPOCH 3:, train_loss =  0.02384, valid_loss =  0.03270 acc@1 =  0.53200 APd =  0.53384 APe =  0.56815 RMSED =  0.16080 WD =  0.09613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:13<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.536\n",
      "EPOCH 4:, train_loss =  0.01747, valid_loss =  0.02917 acc@1 =  0.53600 APd =  0.59517 APe =  0.60443 RMSED =  0.14939 WD =  0.08581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:13<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.592\n",
      "EPOCH 5:, train_loss =  0.01401, valid_loss =  0.02909 acc@1 =  0.59200 APd =  0.61341 APe =  0.61605 RMSED =  0.14657 WD =  0.08193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:13<00:00, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.604\n",
      "EPOCH 6:, train_loss =  0.01050, valid_loss =  0.02721 acc@1 =  0.60400 APd =  0.62335 APe =  0.63999 RMSED =  0.14092 WD =  0.07817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:13<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.604\n",
      "EPOCH 7:, train_loss =  0.00937, valid_loss =  0.02808 acc@1 =  0.57200 APd =  0.60711 APe =  0.63569 RMSED =  0.14465 WD =  0.08044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:13<00:00, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.604\n",
      "EPOCH 8:, train_loss =  0.00771, valid_loss =  0.02967 acc@1 =  0.58000 APd =  0.59982 APe =  0.62493 RMSED =  0.14807 WD =  0.07970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:13<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.604\n",
      "EPOCH 9:, train_loss =  0.00697, valid_loss =  0.02907 acc@1 =  0.56800 APd =  0.60556 APe =  0.62528 RMSED =  0.14675 WD =  0.07865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:13<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score = 0.604\n",
      "EPOCH 10:, train_loss =  0.00610, valid_loss =  0.02745 acc@1 =  0.58800 APd =  0.62383 APe =  0.64091 RMSED =  0.14166 WD =  0.07837\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "train_loop(model, optimizer, criterion, train_loader, valid_loader, CFG.epochs, scheduler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      test_loss = []\n",
    "      preds = []\n",
    "      y = []\n",
    "      for step, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs = collate(inputs)\n",
    "        # move inputs to device\n",
    "        mask = inputs['attention_mask'].to(CFG.device)\n",
    "        input_id = inputs['input_ids'].squeeze(1).to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "          y_preds = model(input_id, mask)\n",
    "          loss = criterion(y_preds, labels)\n",
    "        preds.append(y_preds.detach().cpu())\n",
    "        y.append(labels.detach().cpu())\n",
    "        # calculate loss\n",
    "        test_loss.append(loss.detach().cpu().item())\n",
    "    test_loss = np.mean(test_loss)\n",
    "    preds = torch.concat(preds, dim=0)\n",
    "    y = torch.concat(y, dim=0)\n",
    "    metrics = calculate_metrics(y, preds)\n",
    "\n",
    "    print(f'Test metrics: test_loss={test_loss: 0.5f}', *[f'{name} = {value: 0.5f}' for name, value in metrics.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TrainDataset(CFG, test_df)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: test_loss= 0.02745 acc@1 =  0.58800 APd =  0.62383 APe =  0.64091 RMSED =  0.14166 WD =  0.07837\n"
     ]
    }
   ],
   "source": [
    "test(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: test_loss= 0.02760 acc@1 =  0.63600 APd =  0.66730 APe =  0.66488 RMSED =  0.13617 WD =  0.07353\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
